import os
import re
from openai import OpenAI
from utils.constants import EMBEDDING_MODEL, LLM_MODEL, RAG, ENABLE_MCTS, OUTPUT_PATH
from utils.utils import log_gpt_response, log_update
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import json
from datetime import datetime

# Environment variables
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
)

if 'llama' in LLM_MODEL:
    client = OpenAI(
        api_key=os.getenv("LLAMA_API_KEY"),
        base_url = "https://api.llama-api.com",
    )

embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

def request_gpt_rag(system_content, user_contents, assistant_content, temperature):
    '''
    Function to make an API call to GPT-4

    Parameters:
    - system_content: string containing the system information
    - user_contents: list of strings containing the user inputs
    - assistant_content: list of strings containing the assistant responses
    - temperature: Float (0-1) controlling GPT-4's output randomness.

    Returns:
    - matches: string containing the options file generated by GPT-4
    '''
    log_update("[GPTR] Using RAG")
    print("[GPTR] Using RAG")
    # Load vectorstore and create retriever
    vectorstore = FAISS.load_local("vectorstore_2", embeddings=embeddings, allow_dangerous_deserialization=True)
    retriever = vectorstore.as_retriever()

    # Create LLM and RAG chains
    llm = ChatOpenAI(
        model=LLM_MODEL, 
        temperature=temperature
    )

    # Separate User content
    last_user_content = user_contents[-1]
    user_contents = user_contents[:-1]

    # Create chat prompt template for system and user content
    prompt = [
        ("system", system_content + "\n\n{context}"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}")
    ]
    
    # Now append user and assistant content
    history = []
    if assistant_content:
        for i in range(max(len(user_contents), len(assistant_content))):
            if i < len(user_contents):
                history.append(HumanMessage(content=user_contents[i]))
            if i < len(assistant_content):
                history.append(AIMessage(content=assistant_content[i]))
    else:
        for content in user_contents:
            history.append(HumanMessage(content=content))

    # Create prompt template and RAG
    prompt_template = ChatPromptTemplate.from_messages(prompt)
    qna_chain = create_stuff_documents_chain(llm, prompt_template)
    rag_chain = create_retrieval_chain(retriever, qna_chain)
    
    # Set user question and ask the RAG
    inputs = {
        "history": history,
        "input": last_user_content
    }
    response = rag_chain.invoke(inputs)
    answer = response.get("answer", "")
    
    # Use for Logging the request 
    messages = [("system", system_content)]
    if assistant_content:
        for i in range(max(len(user_contents), len(assistant_content))):
            if i < len(user_contents):
                messages.append(("user", user_contents[i]))
            if i < len(assistant_content):
                messages.append(("assistant", assistant_content[i]))
    else:
        for content in user_contents:
            messages.append(("user", content))
    messages.append(("user", last_user_content))
    log_gpt_response(messages, answer)
    
    # Use regex to extract content between code blocks if present
    matches = re.match(r"(.*?)```(.*?)```(.*)", answer, re.DOTALL)
    
    if matches:
        return matches
    else:
        # Handle invalid response
        with open("invalid_assistant_reply.txt", "a") as file:
            file.write(answer + "\n\n" + "-" * 150 + "\n\n")
        return None


def request_gpt(system_content, user_contents, assistant_content, temperature):
    '''
    Function to make an API call to GPT-4

    Parameters:
    - system_content: string containing the system information
    - chunk_string: string containing the chunk of the options file
    - previous_option_files: list of tuples containing the previous option files and their benchmark results
    - temperature: Float (0-1) controlling GPT-4's output randomness.
    - average_cpu_used: Float indicating average CPU usage (default -1.0).
    - average_mem_used: Float indicating average memory usage (default -1.0).
    - test_name: String stating the benchmark test.

    Returns:
    - matches: string containing the options file generated by GPT-4
    '''

    if RAG:
        return request_gpt_rag(system_content, user_contents, assistant_content, temperature)

    messages = [{"role": "user", "content": system_content}]

    if assistant_content:
        for i in range(max(len(user_contents), len(assistant_content))):
            if i < len(user_contents):
                messages.append({"role": "user", "content": user_contents[i]})
            if i < len(assistant_content):
                messages.append({"role": "assistant", "content": assistant_content[i]})
    else:
        for content in user_contents:
            messages.append({"role": "user", "content": content})


    # Assuming 'client' is already defined and authenticated for GPT-4 API access
    # completion = client.chat.completions.create(
    #     model=LLM_MODEL,
    #     messages=messages,
    #     temperature=temperature,
    #     max_completion_tokens=4096,
    #     frequency_penalty=0,
    #     presence_penalty=0,
    # )

    completion = client.chat.completions.create(
        model=LLM_MODEL,
        messages=messages,
    )

    # Extract the assistant's reply
    assistant_reply = completion.choices[0].message.content

    if ENABLE_MCTS:
        pattern = re.compile(r"```([\s\S]*?)```")
        matches = pattern.findall(assistant_reply)
    else:
        matches = re.match(r"([\s\S]*)```([\s\S]*)```([\s\S]*)", assistant_reply)

    # Log the request and response
    log_gpt_response(messages, assistant_reply)

    # Check if result is good
    if matches is not None:
        return matches 

    # Invalid response
    with open("invalid_assistant_reply.txt", "a") as file:
        file.write(assistant_reply + "\n\n" + "-" * 150 + "\n\n")
    return None

def send_gpt_request(system_contents, user_contents, temperature):
    '''
    Function to send a request to GPT-4

    Parameters:
    - user_contents: list of strings containing the user inputs
    - temperature: Float (0-1) controlling GPT-4's output randomness.

    Returns:
    - matches: string containing the options file generated by GPT-4
    '''

    messages = [{"role": "user", "content": system_contents}, 
                {"role": "user", "content": user_contents}]

    # completion = client.chat.completions.create(
    #     model=LLM_MODEL,
    #     messages=messages,
    #     temperature=1,
    #     max_completion_tokens=4096,
    #     frequency_penalty=0,
    #     presence_penalty=0,
    # )

    completion = client.chat.completions.create(
        model=LLM_MODEL,
        messages=messages,
    )

    # Extract the assistant's reply
    assistant_reply = completion.choices[0].message.content
    log_gpt_response(messages, assistant_reply)

    return assistant_reply




def request_gpt_with_json_response(system_content, user_contents, assistant_content, temperature):
    '''
    Function to make an API call to GPT-4

    Parameters:
    - system_content: string containing the system information
    - chunk_string: string containing the chunk of the options file
    - previous_option_files: list of tuples containing the previous option files and their benchmark results
    - temperature: Float (0-1) controlling GPT-4's output randomness.
    - average_cpu_used: Float indicating average CPU usage (default -1.0).
    - average_mem_used: Float indicating average memory usage (default -1.0).
    - test_name: String stating the benchmark test.

    Returns:
    - response: python dictionary containing the JSON response from GPT-4
    '''
    
    messages = [{"role": "system", "content": "You are a helpful assistant."}]
    if system_content is not None:
        messages.append({"role": "user", "content": system_content})

    if assistant_content:
        for i in range(max(len(user_contents), len(assistant_content))):
            if i < len(user_contents):
                messages.append({"role": "user", "content": user_contents[i]})
            if i < len(assistant_content):
                messages.append({"role": "assistant", "content": assistant_content[i]})
    else:
        for content in user_contents:
            messages.append({"role": "user", "content": content})



    we_did_not_specify_stop_tokens = True
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=messages,
            temperature=temperature,
            response_format={"type": "json_object"},
        )
        print(response.choices[0].message)
        print(response.choices[0])
        # Check if the conversation was too long for the context window, resulting in incomplete JSON 
        if response.choices[0].finish_reason == "length":
            raise Exception("The conversation was too long for the context window, resulting in incomplete JSON")
            pass

        # Check if the OpenAI safety system refused the request and generated a refusal instead
        if response.choices[0].message.refusal is not None:
            # your code should handle this error case
            # In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing
            print(response.choices[0].message.refusal)
            raise Exception("The OpenAI safety system refused the request and generated a refusal instead")

        # Check if the model's output included restricted content, so the generation of JSON was halted and may be partial
        if response.choices[0].finish_reason == "content_filter":
            # your code should handle this error case
            raise Exception("The model's output included restricted content, so the generation of JSON was halted and may be partial")
            pass

        if response.choices[0].finish_reason == "stop":
            # In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a "stop token"

            if we_did_not_specify_stop_tokens:
                # If you didn't specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object
                # This will parse successfully
                print(response.choices[0].message.content)
            else:
                # Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately
                pass
    except Exception as e:
        # Your code should handle errors here, for example a network error calling the API
        print(e)
        raise e
    
    try:
        json_dict = json.loads(response.choices[0].message.content)
    except Exception as e:
        print("Error parsing JSON response from GPT-4")
        raise e
    return json_dict


def request_gpt_with_structured_output(system_content, user_contents, assistant_content, response_format, temperature):
    '''
    Function to make an API call to GPT-4

    Parameters:
    - system_content: string containing the system information
    - chunk_string: string containing the chunk of the options file
    - previous_option_files: list of tuples containing the previous option files and their benchmark results
    - temperature: Float (0-1) controlling GPT-4's output randomness.
    - average_cpu_used: Float indicating average CPU usage (default -1.0).
    - average_mem_used: Float indicating average memory usage (default -1.0).
    - test_name: String stating the benchmark test.

    Returns:
    - response: python dictionary containing the JSON response from GPT-4
    '''
    
    messages = [{"role": "system", "content": "You are a helpful assistant."}]
    if system_content is not None:
        messages.append({"role": "user", "content": system_content})

    if assistant_content:
        for i in range(max(len(user_contents), len(assistant_content))):
            if i < len(user_contents):
                messages.append({"role": "user", "content": user_contents[i]})
            if i < len(assistant_content):
                messages.append({"role": "assistant", "content": assistant_content[i]})
    else:
        for content in user_contents:
            messages.append({"role": "user", "content": content})



    we_did_not_specify_stop_tokens = True
    try:
        response = client.beta.chat.completions.parse(
            model=LLM_MODEL,
            messages=messages,
            temperature=temperature,
            response_format=response_format,
        )
        # if not exist file add header
        cost_log_file_path = os.path.join(OUTPUT_PATH, "gpt_cost.txt")
        if not os.path.exists(cost_log_file_path):
            with open(cost_log_file_path, "w") as file:
                file.write("timestamp\tmodel\ttokens\tprompt_tokens\tcompletion_tokens\tcached_tokens\n")
        with open(cost_log_file_path, "a") as file:
            timestamp_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            file.write(f"{timestamp_str}\t{LLM_MODEL}\t{response.usage.total_tokens}\t{response.usage.prompt_tokens}\t{response.usage.completion_tokens}\t{response.usage.prompt_tokens_details.cached_tokens}\n")
            
        # print(response.choices[0].message)
        # print(response.choices[0])
        # Check if the conversation was too long for the context window, resulting in incomplete JSON 
        if response.choices[0].finish_reason == "length":
            raise Exception("The conversation was too long for the context window, resulting in incomplete JSON")
            pass

        # Check if the OpenAI safety system refused the request and generated a refusal instead
        if response.choices[0].message.refusal is not None:
            # your code should handle this error case
            # In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing
            print(response.choices[0].message.refusal)
            raise Exception("The OpenAI safety system refused the request and generated a refusal instead")

        # Check if the model's output included restricted content, so the generation of JSON was halted and may be partial
        if response.choices[0].finish_reason == "content_filter":
            # your code should handle this error case
            raise Exception("The model's output included restricted content, so the generation of JSON was halted and may be partial")
            pass

        if response.choices[0].finish_reason == "stop":
            # In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a "stop token"

            if we_did_not_specify_stop_tokens:
                # If you didn't specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object
                # This will parse successfully
                print(response.choices[0].message.content)
            else:
                # Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately
                pass
    except Exception as e:
        # Your code should handle errors here, for example a network error calling the API
        print(e)
        raise e
    
    return response.choices[0].message.parsed